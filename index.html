<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
    <title>Akshita Gupta</title>

    <meta content="Akshita Gupta" name="author">
    <meta content="width=device-width, initial-scale=1" name="viewport">

    <link href="stylesheet.css" rel="stylesheet" type="text/css">
    <link href="images/seal_icon.png" rel="icon" type="image/png">
    <style>
        /* Apply Inter font to the entire page */
        body {
            font-family: 'calibri', sans-serif;
        }
        #myimg {
            width: 100%;
            max-width: 100%;
            border-radius: 50%;
            border: 1px solid #ddd;
            padding: 5px;
        }
        p {
            line-height: 1.7;
            font-size: 16px;
        }
        ul li {
            font-size: 16px;
            line-height: 1.6;
        }
</style>

</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <!-- <p style="text-align:center"> -->
                        <!-- <name>Akshita Gupta</name> -->
                        <p align="center" id="namechange">
                            <span id="a"><name>Akshita Gupta</name></span><span id="b" style="font-family: 'Gugi', cursive; font-size: 40px;">‡§Ö‡§ï‡•ç‡§∑‡§ø‡§§‡§æ ‡§ó‡•Å‡§™‡•ç‡§§‡§æ </span>
                        </p>
                        <p style="text-align:justify">
                            I am an ELLIS PhD student at <span style="color:#0071C5;">TU Darmstadt</span>, co-supervised
                            by <a href="https://rohrbach.vision/">Prof. Marcus Rohrbach</a> and <a
                                href="https://federicotombari.github.io/">Dr. Federico Tombari</a> at <span
                                style="color:#DB4437;">Google Zurich</span>. I completed my MASc at the <span
                                style="color:#8C1D40;">University of Guelph</span>, where I was advised by <a
                                href="https://www.gwtaylor.ca/">Prof. Graham Taylor</a>. During that time, I was also a
                            student researcher at the <a href="https://vectorinstitute.ai/"><span
                                style="color:#0047AB;">Vector Institute</span></a>.
                        </p>
                        <p style="text-align:justify">
                            I was fortunate to spend time as a research intern at <span
                                style="color:#A2AAAD;">Apple</span> under <a
                                href="https://scholar.google.com/citations?user=x7Z3ysQAAAAJ&hl=ru">Dr. Tatiana
                            Likhomanenko</a>, <span style="color:#00A4EF;">Microsoft</span> under <a
                                href="https://g1910.github.io/">Gaurav Mittal</a> and <a
                                href="https://www.microsoft.com/en-us/research/people/meic/">Mei Chen</a>, <span
                                style="color:#0047AB;">Vector Institute</span> under <a
                                href="https://sites.google.com/view/dbemerson">Dr. David Emerson</a>, and as a scientist
                            in residence at <span style="color:#000000;">NextAI</span> with Prof. Graham Taylor.
                        </p>
                        <p style="text-align:justify">
                            Before coming to academia, I worked as a Data Scientist at <a
                                href="https://space42.ai/en"><span style="color:#F25022;">Bayanat</span></a>, where I
                            focused on projects related to detection and segmentation. Prior to that, I was a Research
                            Engineer at the <span style="color:#009688;">Inception Institute of Artificial Intelligence (IIAI)</span>,
                            working with <a href="https://sites.google.com/view/sanath-narayan">Dr. Sanath Narayan</a>,
                            <a href="https://salman-h-khan.github.io/">Dr. Salman Khan</a>, and <a
                                href="https://sites.google.com/view/fahadkhans/home">Dr. Fahad Shahbaz Khan</a>.
                        </p>
                        <p style="text-align:center">
                            <a href="mailto:akshita.sem.iitr@gmail.com">Email</a> &nbsp/&nbsp
                            <a href="https://scholar.google.com/citations?user=G01YeI0AAAAJ&hl=en">Google Scholar</a>
                            &nbsp/&nbsp
                            <a href="https://twitter.com/akshitac8">Twitter</a> &nbsp/&nbsp
                            <a href="https://github.com/akshitac8">Github</a> &nbsp/&nbsp
                            <a href="https://akshitac8.github.io/Gupta_Akshita_resume-12.pdf">Resume/CV</a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="images/profile_aks.png"><img alt="profile photo" class="hoverZoomLink"
                                                              id="myimg" src="images/profile_aks.png"></a>
                    </td>
                </tr>
                </tbody>
            </table>

            <h2 style="text-align:left; margin-left: 10px;">What's New</h2>
            <div class="news-container">
                <table style="width:100%;border:0px;border-spacing:4px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <tr>
                        <td><strong>[Mar 2025]</strong></td>
                        <td>Excited to be an ELLIS PhD student at <strong>TU Darmstadt</strong> under Prof. Marcus
                            Rohrbach and Dr. Federico Tombari (Google Zurich) üéâ
                        </td>
                    </tr>
                    <tr>
                        <td><strong>[Mar 2025]</strong></td>
                        <td>Graduated and Defended my <a
                                href="https://atrium.lib.uoguelph.ca/items/67a35868-ca5a-494f-9116-62ea1c57b733">Masters
                            Thesis</a></td>
                    </tr>
                    <tr>
                        <td><strong>[Nov 2024]</strong></td>
                        <td>Our paper <a href="https://arxiv.org/pdf/2411.17690">Visatronic: A Multimodal Decoder-Only
                            Model for Speech Synthesis</a> is now on <strong>ArXiv 2025</strong>!
                        </td>
                    </tr>
                    <tr>
                        <td><strong>[Jun 2024]</strong></td>
                        <td>Joined <strong>Apple</strong> as a Research Intern</td>
                    </tr>
                    <tr>
                        <td><strong>[May 2024]</strong></td>
                        <td>Serving as a Scientist-in-Residence at <strong>NextAI</strong>.</td>
                    </tr>
                    <tr>
                        <td><strong>[Jan 2024]</strong></td>
                        <td>Our paper <a href="https://arxiv.org/abs/2404.01282">Long-Short-range Adapter for Scaling
                            End-to-End Temporal Action Localization</a> is accepted at <strong>WACV 2025 (<span
                                style="color:red;">Oral</span>)</strong>! üé§
                        </td>
                    </tr>
                    <tr>
                        <td><strong>[Dec 2023]</strong></td>
                        <td>Our work <a href="https://arxiv.org/pdf/2406.15556">Open-Vocabulary Temporal Action
                            Localization using Multimodal Guidance</a> is accepted at <strong>BMVC 2024</strong>!
                        </td>
                    </tr>
                    <tr>
                        <td><strong>[Jun 2023]</strong></td>
                        <td>Our paper <a href="https://arxiv.org/pdf/2101.11606.pdf"> Generative Multi-Label Zero-Shot
                            Learning </a> is accepted at TPAMI 2023.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>[Jun 2023]</strong></td>
                        <td>Started interning at Microsoft, ROAR team.</td>
                    </tr>
                    <tr>
                        <td><strong>[Jan 2023]</strong></td>
                        <td>Interned at Vector Institute with AI Eng team.</td>
                    </tr>
                    <tr>
                        <td><strong>[Sep 2022]</strong></td>
                        <td>Joined Prof. Graham Taylor's Lab and Vector Institute.</td>
                    </tr>
                    <tr>
                        <td><strong>[Mar 2022]</strong></td>
                        <td>OW-DETR accepted at CVPR 2022.</td>
                    </tr>
                    <tr>
                        <td><strong>[Sep 2021]</strong></td>
                        <td>Reviewer for CVPR 2023, CVPR 2022, ECCV 2022, ICCV 2021, TPAMI.</td>
                    </tr>
                    <tr>
                        <td><strong>[Jul 2021]</strong></td>
                        <td>BiAM accepted at ICCV 2021.</td>
                    </tr>
                    <tr>
                        <td><strong>[Feb 2021]</strong></td>
                        <td>Serving as a reviewer for ML Reproducibility Challenge 2020.</td>
                    </tr>
                    <tr>
                        <td><strong>[Jan 2021]</strong></td>
                        <td>Paper out on arxiv: <a href="https://arxiv.org/pdf/2101.11606.pdf"> Generative Multi-Label
                            Zero-Shot Learning </a></td>
                    </tr>
                    <tr>
                        <td><strong>[Jul 2020]</strong></td>
                        <td>TF-VAEGAN accepted at ECCV 2020.</td>
                    </tr>
                    <tr>
                        <td><strong>[Aug 2019]</strong></td>
                        <td>A Large-scale Instance Segmentation Dataset for Aerial Images (iSAID) is available for <a
                                href="https://captain-whu.github.io/iSAID/index.html"> download </a>.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>[Aug 2018]</strong></td>
                        <td>One paper accepted at Interspeech, chime workshop 2018.</td>
                    </tr>
                    <tr>
                        <td><strong>[May 2018]</strong></td>
                        <td>Selected as an Outreachy intern, with Mozilla.</td>
                    </tr>
                    </tbody>
                </table>
            </div>
            <h1></h1>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="width:100%;vertical-align:middle">
                        <heading>Research Interest</heading>
                        <p>
                            I'm interested in developing models which can learn with limited data and few, zero or one training sample(s).
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/visatronic.png' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/pdf/2411.17690">
                            <papertitle> Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis</papertitle>
                        </a>
                        <br>
                        <strong>Akshita Gupta</strong>,
                        <a href="https://scholar.google.com/citations?user=x7Z3ysQAAAAJ&hl=ru">Tatiana Likhomanenko<sup>*</sup></a>,
                        <a href="https://karreny.github.io">Karren Dai Yang </a>,
                        <a href="https://richardbaihe.github.io"> Richard He Bai</a><br>
                        <a href="https://scholar.google.com/citations?user=1AHzh04AAAAJ&hl=en">Zakaria Aldeneh</a>
                        <a href="https://scholar.google.com/citations?user=kjMNMLkAAAAJ&hl=en">Navdeep Jaitly</a>
                        <br>
                        <strong>Arxiv 2025 </strong>
                        <br>
                        <a href="https://arxiv.org/pdf/2411.17690">paper</a> /
                        <a href="https://github.com/akshitac8/tfvaegan">code</a>
                        <ul>
                            <li>
                                <u>Description:</u>
                            </li>
                            <li>
                                <u>Outcome:</u>
                            </li>
                        </ul>
                    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/feedback_vis.png' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/pdf/2404.01282">
                            <papertitle> LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action
                                Localization
                            </papertitle>
                        </a>
                        <br>
                        <a href="https://sites.google.com/view/sanath-narayan">Akshita Gupta<sup>*</sup></a>,
                        <strong>Akshita Gupta<sup>*</sup></strong>,
                        <a href="https://salman-h-khan.github.io/">Salman </a>,
                        <a href="https://sites.google.com/view/fahadkhans/home"> Shahbaz Khan,</a><br>
                        <a href="https://www.ceessnoek.info/">Cees G. M. Snoek,</a>
                        <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en">Ling Shao,</a>
                        <br>
                        <strong>WACV 2025 </strong>
                        <br>
                        <a href="https://arxiv.org/abs/2003.07833">paper</a> /
                        <a href="https://github.com/akshitac8/tfvaegan">code</a>
                        <ul>
                            <li>
                                <u>Description:</u> Developed a generative feature synthesizing framework for zero-shot
                                learning.
                            </li>
                            <li>
                                <u>Outcome:</u> Improved state-of-the-art performances on CUB, FLO, SUN, and AWA by
                                4.6%, 7.1%, 1.7%, and 3.1% harmonic mean by enforcing semantic consistency at all stages
                                of zero-shot learning.
                            </li>
                        </ul>
                    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/feedback_vis.png' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/pdf/2406.15556">
                            <papertitle> Open-Vocabulary Temporal Action
                                Localization using Multimodal Guidance
                            </papertitle>
                        </a>
                        <br>
                        <a href="https://sites.google.com/view/sanath-narayan">Akshita Gupta<sup>*</sup></a>,
                        <strong>Akshita Gupta<sup>*</sup></strong>,
                        <a href="https://salman-h-khan.github.io/">Salman </a>,
                        <a href="https://sites.google.com/view/fahadkhans/home"> Shahbaz Khan,</a><br>
                        <a href="https://www.ceessnoek.info/">Cees G. M. Snoek,</a>
                        <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en">Ling Shao,</a>
                        <br>
                        <strong>BMVC 2024 </strong>
                        <br>
                        <a href="https://arxiv.org/abs/2003.07833">paper</a> /
                        <a href="https://github.com/akshitac8/tfvaegan">code</a>
                        <ul>
                            <li>
                                <u>Description:</u> Developed a generative feature synthesizing framework for zero-shot
                                learning.
                            </li>
                            <li>
                                <u>Outcome:</u> Improved state-of-the-art performances on CUB, FLO, SUN, and AWA by
                                4.6%, 7.1%, 1.7%, and 3.1% harmonic mean by enforcing semantic consistency at all stages
                                of zero-shot learning.
                            </li>
                        </ul>
                    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/feedback_vis.png' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/pdf/2101.11606">
                            <papertitle> Generative Multi-Label Zero-Shot Learning</papertitle>
                        </a>
                        <br>
                        <a href="https://sites.google.com/view/sanath-narayan">Akshita Gupta<sup>*</sup></a>,
                        <strong>Akshita Gupta<sup>*</sup></strong>,
                        <a href="https://salman-h-khan.github.io/">Salman </a>,
                        <a href="https://sites.google.com/view/fahadkhans/home"> Shahbaz Khan,</a><br>
                        <a href="https://www.ceessnoek.info/">Cees G. M. Snoek,</a>
                        <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en">Ling Shao,</a>
                        <br>
                        <strong>TPAMI 2023 </strong>
                        <br>
                        <a href="https://arxiv.org/abs/2003.07833">paper</a> /
                        <a href="https://github.com/akshitac8/tfvaegan">code</a>
                        <ul>
                            <li>
                                <u>Description:</u> Developed a generative feature synthesizing framework for zero-shot
                                learning.
                            </li>
                            <li>
                                <u>Outcome:</u> Improved state-of-the-art performances on CUB, FLO, SUN, and AWA by
                                4.6%, 7.1%, 1.7%, and 3.1% harmonic mean by enforcing semantic consistency at all stages
                                of zero-shot learning.
                            </li>
                        </ul>
                    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <!-- <td style="padding:20px;width:25%;vertical-align:middle"> -->
                    <td style="vertical-align:middle">
                        <div class="one">
                            <img src='images/OWDETR_intro.png' width="200">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://akshitac8.github.io/OWDETR">
                            <papertitle>OW-DETR: Open-world Detection Transformer</papertitle>
                        </a>
                        <br>
                        <a href="https://sites.google.com/view/sanath-narayan">Akshita Gupta<sup>*</sup></a>,
                        <strong>Sanath Narayan<sup>*</sup></strong>,
                        <a href="https://josephkj.in">Joseph KJ</a>,
                        <a href="https://salman-h-khan.github.io/">Salman Khan</a>,
                        <a href="https://sites.google.com/view/fahadkhans/home">Fahad Shahbaz Khan,</a><br>
                        <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
                        <br>
                        <!--               (* denotes equal contribution)
                         --> <strong>CVPR 2022 </strong>
                        <br>
                        <a href="https://arxiv.org/pdf/2112.01513.pdf">paper</a> /
                        <a href="https://github.com/akshitac8/OW-DETR">code</a>
                        <ul>
                            <li>
                                <u>Description:</u> Developed multi-scale context aware detection framework with
                                attention-driven psuedo-labelling.
                            </li>
                            <li>
                                <u>Outcome:</u> Improved state-of-the-art performances on MS-COCO dataset with absolute
                                gains ranging from 1.8% to 3.3% in terms of unknown recall.
                            </li>
                        </ul>
                    </td>
                </tr>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <tr>
                        <!-- <td style="padding:20px;width:25%;vertical-align:middle"> -->
                        <td style="vertical-align:middle">
                            <div class="one">
                                <img src='images/image834.png' width="200">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <a href="https://akshitac8.github.io/BiAM">
                                <papertitle>Discriminative Region-based Multi-Label Zero-Shot Learning</papertitle>
                            </a>
                            <br>
                            <a href="https://sites.google.com/view/sanath-narayan">Sanath Narayan<sup>*</sup></a>,
                            <strong>Akshita Gupta<sup>*</sup></strong>,
                            <a href="https://salman-h-khan.github.io/">Salman Khan</a>,
                            <a href="https://sites.google.com/view/fahadkhans/home">Fahad Shahbaz Khan,</a><br>
                            <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en">Ling Shao,</a>
                            <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
                            <br>
                            <!--               (* denotes equal contribution)
                             --> <strong>ICCV 2021 </strong>
                            <br>
                            <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Narayan_Discriminative_Region-Based_Multi-Label_Zero-Shot_Learning_ICCV_2021_paper.pdf">paper</a>
                            /
                            <a href="https://github.com/akshitac8/BiAM">code</a>
                            <ul>
                                <li>
                                    <u>Description:</u> Developed a attention module which combines both region-level
                                    and global-level contextual information.
                                </li>
                                <li>
                                    <u>Outcome:</u> Improved state-of-the-art performances on NUS-WIDE, OpenImages by
                                    6.9% and 31.9% mAP score.
                                </li>
                            </ul>
                        </td>
                    </tr>
                    </tbody>
                </table>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img align="right" src='images/cvpr_result.png' width="140">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <a href="https://akshitac8.github.io/GAN_MLZSL">
                                <papertitle>Generative Multi-Label Zero-Shot Learning</papertitle>
                            </a>
                            <br>
                            <strong>Akshita Gupta<sup>*</sup></strong>,
                            <a href="https://sites.google.com/view/sanath-narayan">Sanath Narayan<sup>*</sup></a>,
                            <a href="https://salman-h-khan.github.io/">Salman Khan</a>,
                            <a href="https://sites.google.com/view/fahadkhans/home">Fahad Shahbaz Khan,</a><br>
                            <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en">Ling Shao,</a>
                            <a href="http://www.cvc.uab.es/LAMP/joost/">Joost van de Weijer</a>
                            <br>
                            <strong> Under Review in TPAMI </strong>
                            <br>
                            <a href="https://arxiv.org/abs/2003.07833">paper</a> /
                            <a href="https://github.com/akshitac8/Generative_MLZSL">code</a>
                            <ul>
                                <li>
                                    <u>Description:</u> Developed a generative model that constructs multi-label
                                    features for (generalized) zero-shot learning.
                                </li>
                                <li>
                                    <u>Outcome:</u> Improved state-of-the-art performances on NUS-WIDE, OpenImages and
                                    MS-COCO by 3.3%, 4.3% and 15.7% mAP score.
                                </li>
                            </ul>
                        </td>
                    </tr>
                    </tbody>
                </table>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/feedback_vis.png' width="160">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <a href="https://akshitac8.github.io/tfvaegan/">
                                <papertitle>Latent Embedding Feedback and Discriminative Features for Zero-Shot
                                    Classification
                                </papertitle>
                            </a>
                            <br>
                            <a href="https://sites.google.com/view/sanath-narayan">Sanath Narayan<sup>*</sup></a>,
                            <strong>Akshita Gupta<sup>*</sup></strong>,
                            <a href="https://salman-h-khan.github.io/">Salman Khan</a>,
                            <a href="https://sites.google.com/view/fahadkhans/home">Fahad Shahbaz Khan,</a><br>
                            <a href="https://www.ceessnoek.info/">Cees G. M. Snoek,</a>
                            <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en">Ling Shao,</a>
                            <br>
                            <strong>ECCV 2020 </strong>
                            <br>
                            <a href="https://arxiv.org/abs/2003.07833">paper</a> /
                            <a href="https://github.com/akshitac8/tfvaegan">code</a>
                            <ul>
                                <li>
                                    <u>Description:</u> Developed a generative feature synthesizing framework for
                                    zero-shot learning.
                                </li>
                                <li>
                                    <u>Outcome:</u> Improved state-of-the-art performances on CUB, FLO, SUN, and AWA by
                                    4.6%, 7.1%, 1.7%, and 3.1% harmonic mean by enforcing semantic consistency at all
                                    stages of zero-shot learning.
                                </li>
                            </ul>
                        </td>
                    </tr>
                    </tbody>
                </table>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img align="right" src='images/isaid.png' width="140">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <a href="https://captain-whu.github.io/iSAID/">
                                <papertitle>iSAID: A Large-scale Dataset for Instance Segmentation in Aerial Images
                                </papertitle>
                            </a>
                            <br>
                            <a href="https://scholar.google.es/citations?user=WNGPkVQAAAAJ&hl=en">Syed Waqas Zamir,</a>
                            <a href="https://adityac8.github.io/">Aditya Arora,</a>
                            <strong>Akshita Gupta</strong>,
                            <a href="https://salman-h-khan.github.io/">Salman Khan,</a>
                            <a href="https://scholar.google.ae/citations?user=qd8Blw0AAAAJ&hl=en">Guolei Sun,</a>
                            <a href="https://sites.google.com/view/fahadkhans/home">Fahad Shahbaz Khan,</a>
                            <a href="https://scholar.google.com/citations?user=vD-ezyQAAAAJ&hl=en">Fan Zhu,</a>
                            <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en">Ling Shao,</a>
                            <a href="http://www.captain-whu.com/xia_En.html">Gui-Song Xia,</a>
                            <a href="https://scholar.google.com/citations?user=UeltiQ4AAAAJ&hl=en">Xiang Bai</a>
                            <br>
                            <strong>CVPR Workshop 2019 <font color="red">(Oral Presentation)</font></strong>
                            <br>
                            <a href="https://github.com/CAPTAIN-WHU/iSAID_Devkit">code</a> /
                            <a href="https://captain-whu.github.io/iSAID/index.html">dataset</a>
                            <ul>
                                <li>
                                    <u>Description:</u> Improved state of the art object detector (Mask-RCNN and PANet)
                                    for aerial imagery.
                                </li>
                                <li>
                                    <u>Outcome:</u> Proposed a large scale instance segmentation and object detection
                                    dataset (iSAID) with benchmarking on mask-RCNN and PANet.
                                </li>
                            </ul>
                        </td>
                    </tr>
                    </tbody>
                </table>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/interspeech.png' width="160">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <a href="https://arxiv.org/abs/1811.00936">
                                <papertitle>Acoustic features fusion using attentive multi-channel deep architecture
                                </papertitle>
                            </a>
                            <br>
                            <a href="http://deeplearn-ai.com/about-3/?i=1">Gaurav Bhatt,</a>
                            <strong>Akshita Gupta</strong>,
                            <a href="https://adityac8.github.io/">Aditya Arora,</a>
                            <a href="http://bala.cs.faculty.iitr.ac.in/">Balasubramanian Raman</a>
                            <br>
                            <strong>Interspeech Workshop 2018 </strong>
                            <br>
                            <a href="https://github.com/DeepLearn-lab/Acoustic-Feature-Fusion_Chime18">code</a>
                            <ul>
                                <li>
                                    <u>Description:</u> Developed an attention based framework for acoustic scene
                                    recognition and audio tagging.
                                </li>
                                <li>
                                    <u>Outcome:</u> Improved the equal error rate by atleast 3% over the Dcase challenge
                                    results.
                                </li>
                            </ul>
                        </td>
                    </tr>
                    </tbody>
                </table>
                <table align="center" border="0" cellpadding="20" cellspacing="0" width="100%">
                    <tr>
                        <td>
                            <br>
                            <p align="right">
                                <font size="2">
                                    <strong>I borrowed this website layout from <a href="https://jonbarron.info/"
                                                                                   target="_blank">here</a>!</strong>
                                </font>
                            </p>
                        </td>
                    </tr>
                </table>
        </td>
    </tr>
</table>
</body>

</html>
